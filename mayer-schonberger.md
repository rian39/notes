Big data is not about trying to "teach" a computer to "think" like humans. Instead, it's about applying math to huge quantities of data in order to infer probabilities. 12

Looking at vastly more data also permits us to loosen up our desire for exactitude ... It's a tradeoff: with less error from sampling we can accept more from measurement error 13

using N= all means we can drill down deep into data; samples can't do that nearly as well. ... Often, the really interesting things in life are found in places that samples fail to catch. 27

Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample. 27

For a long time, random sampling was a good shortcut. It made analysis of large data problems possible in the pre-digital era. ... Having the full (or close to the full) dataset provides a lot more freedom to explore, to look at the data from different angles or to look closer at certain aspects of it. 29

One of the areas that being most dramatically shaken up by N = all is the social sciences. They have lost their monopoly on makes sense of empirical social data, as big-data analysis replaces the highly skilled survey specialists of the past. ... More important, the need to sample disappears 30. 

We tend to think of statistical sampling as some sort of immutable bedrock, like the principles of geometry or the laws of gravity. But the concept is less than a century old, and it was devleoped to solve a particular problem at a particular moment in time under specific technologicla constraints. These constraints no longer exist to the same extent. Reaching for a random sample in the age of big data is like clutching at horse whip in the era of the motor car. 31
