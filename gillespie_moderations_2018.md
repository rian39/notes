
> Users cannot continue to expect platforms to be hands-off and expect them to solve problems perfectly and expect them to get with the times and expect them to be impartial and automatic. 197
> Platforms should make a radical commitment to turning the data they already have back to me in a legible and actionable form, everything they could tell me contextually about why a post is there and how I should assess it. We have already paid for this transparency, with our data.199
>The accumulated history users have with a platform—established social networks, a legacy of interactions, an archive of photos, an accumulated matrix of preferences—does in fact discourage them from abandoning it, even when they are dissatisfied with how it governs their use, even when they are fed up, even when they must endure harassment to stay. 200 

For platforms, popularity is one of the most fundamental metrics, often serving as proxy to every other: relevance, merit, newsworthiness. Platforms amplify the popular by returning it to users in the form of recommenda- tions, cued- up videos, trends, and feeds. Harassment and hate take advan- tage of this: cruel insults that classmates will pass around, slurs aimed at women that fellow misogynists will applaud, nonconsensual porn that ap- peals to prurient interests. These are not just attacks, they generate likes, views, comments, and retweets, making it hard for platforms to discern their toxicity or pass up their popularity. With business models that use popular- ity as the core proxy for engagement, too often platforms err on the side of encouraging as many people to stay as possible, imposing rules with the least consequences, keeping troublesome users if they can, and bringing them back quickly if they can’t. 201

In embracing the Internet, the web, and especially social media platforms for public discourse and sociality, we made a Faustian bargain, or a series of them, and we are now facing the sober realities they produced. If we dreamed of free information, we found we also got free advertising. If we dreamed of participation, we also got harassment. If we dreamed of sharing, we also got piracy. If we dreamed of political activism online, we also got clicktivism, political pandering, and tweetstorms. If we dreamed of forming global, decentralized communities of interest, we also got ISIS recruitment. If we dreamed of new forms of public visibility, we also got NSA surveillance. If we dreamed of free content and crowdsourced knowledge, we also got the exploitation of free labor. If we dreamed of easy interconnection between complex technical resources, we also got hacked passwords, data breaches, and cyberwar. 205

These platforms now function at a scale and under a set of expectations that increasingly demands automation. Yet the kinds of decisions that plat- forms must make, especially in content moderation, are precisely the kinds of decisions that should not be automated, and perhaps cannot be. They are judgments of value, meaning, importance, and offense. They depend both on a human revulsion to the horrifi c and a human sensitivity to con- tested cultural values. There is, in many cases, no right answer for whether to allow or disallow, except in relation to specifi c individuals, communities, or nations that have debated and regulated standards of propriety and legal- ity. And even then, the edges of what is considered appropriate are constantly recontested, and the values they represent are always shifting. 206

It is not just, as I hope I have shown, that all platforms moderate, or that all platforms have to mod- erate, or that most tend to disavow it while doing so. It is that moderation, far from being occasional or ancillary, is in fact an essential, constant, and definitional part of what platforms do. I mean this literally: moderation is the essence of platforms, it is the commodity they offer. By this point in the book, this should be plain. First, moderation is a surprisingly large part of what they do, in a practical, day- to- day sense, and in terms of the time, resources, and number of employees they devote to it. Moreover, moderation shapes how platforms conceive of their users—and not just the ones who break rules or seek help. By shifting some of the labor of moderation, through fl agging, platforms deputize users as amateur editors and police. From that moment, platform managers must in part think of, address, and manage users as such. This adds another layer to how users are conceived of, along with seeing them as customers, producers, free labor, and commodity. 207

Content moderation is a key part of what social media platforms do that is different, that distinguishes them from the open web: they moderate (removal, fi ltering, suspension), they recommend (news feeds, trending lists, Gillespie, T. (2018). Custodians of the internet : Platforms, content moderation, and the hidden decisions that personalized suggestions), and they curate (featured content, front- page offerings). Platforms use these three levers together to, actively and dy- namically, tune the unexpected participation of users, produce the “right” feed for each user, the “right” social exchanges, the “right” kind of commu- nity. (“Right” here may mean ethical, legal, and healthy; but it also means whatever will promote engagement, increase ad revenue, and facilitate data collection. And given the immense pushback from users, legislators, and the press, these platforms appear to be deeply out of tune.i 207-208

Rethinking content moderation might begin with this recognition, that
content moderation is the essential offer platforms make, and part of how
they tune the public discourse they purport to host. Platforms could be held
responsible, at least partially, for how they tend to that public discourse, and
to what ends. 209
