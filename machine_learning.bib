


% better-bibtex: generated
@article{Wu_2008,
	title = {{Top 10 algorithms in data mining}},
	volume = {{14}},
	pages = {1–37},
	number = {1},
	journaltitle = {{Knowledge and Information Systems}},
	author = {Wu, X. and Kumar, V. and Ross Quinlan, J. and Ghosh, J. and Yang, Q. and Motoda, H. and McLachlan, G. J and Ng, A. and Liu, B. and Yu, P. S and {others}},
	date = {2008}
}

% better-bibtex: generated
% better-bibtex: soft conflict
@video{StanfordUniversity_2008,
	title = {{Lecture 2 | Machine Learning (Stanford)}},
	url = {http://www.youtube.com/watch?v=5u4G23\_OohI\&feature=youtube\_gdata\_player},
	abstract = {Lecture by Professor Andrew Ng for Machine Learning (CS 229) in the Stanford Computer Science department.  Professor Ng lectures on linear regression, gradient descent, and normal equations and discusses how they relate to machine learning. 

This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include supervised learning, unsupervised learning, learning theory, reinforcement learning and adaptive control.   Recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing are also discussed.

Complete Playlist for the Course:
http://www.youtube.com/view\_play\_list?p=A89DCFA6ADACE599

CCS 229 Course Website:
http://www.stanford.edu/class/cs229/

Stanford University:
http://www.stanford.edu/

Stanford University Channel on YouTube:
http://www.youtube.com/stanford},
	editora = {{StanfordUniversity}},
	editoratype = {collaborator},
	urldate = {2013-02-11},
	date = {2008-07-23}
}

% better-bibtex: generated
@article{_2010,
	title = {{The Grill: Tom Mitchell}},
	url = {http://www.computerworld.com/s/article/346917/The\_Grill\_Tom\_Mitchell},
	shorttitle = {{The Grill}},
	journaltitle = {{Computerworld}},
	urldate = {2012-03-09},
	date = {2010-02-22},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/NKP5QXI2/The_Grill_Tom_Mitchell.html:text/html}
}

% better-bibtex: generated
% better-bibtex: soft conflict
@online{_,
	title = {{The Company for Apache Lucene Solr Open Source Search | Lucid Imagination}},
	url = {http://www.lucidimagination.com/},
	urldate = {2012-03-15}
}

% better-bibtex: generated
@book{Clarke_2010,
	title = {{Information Retrieval: Implementing and Evaluating Search Engines}},
	isbn = {0262026511},
	shorttitle = {{Information Retrieval}},
	pagetotal = {632},
	publisher = {{The MIT Press}},
	author = {Clarke, Charles L. A. and Buettcher, Stefan and Cormack, Gordon V.},
	date = {2010-07-23}
}

% better-bibtex: generated
@book{Manning_2008,
	edition = {{1}},
	title = {{Introduction to Information Retrieval}},
	isbn = {0521865719},
	pagetotal = {496},
	publisher = {{Cambridge University Press}},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	date = {2008-07-07}
}

% better-bibtex: generated
% better-bibtex: soft conflict
@article{Breiman_2001,
	title = {{Random forests}},
	volume = {{45}},
	pages = {5–32},
	number = {1},
	journaltitle = {{Machine learning}},
	author = {Breiman, L.},
	date = {2001}
}

% better-bibtex: generated
@article{Ensmenger_2012,
	title = {{Is Chess the Drosophila of Artificial Intelligence? A Social History of an Algorithm}},
	volume = {{42}},
	issn = {0306-3127, 1460-3659},
	url = {http://sss.sagepub.com.ezproxy.lancs.ac.uk/content/42/1/5},
	doi = {10.1177/0306312711424596},
	shorttitle = {{Is chess the drosophila of artificial intelligence?}},
	abstract = {Since the mid 1960s, researchers in computer science have famously referred to chess as the ‘drosophila’ of artificial intelligence (AI). What they seem to mean by this is that chess, like the common fruit fly, is an accessible, familiar, and relatively simple experimental technology that nonetheless can be used productively to produce valid knowledge about other, more complex systems. But for historians of science and technology, the analogy between chess and drosophila assumes a larger significance. As Robert Kohler has ably described, the decision to adopt drosophila as the organism of choice for genetics research had far-reaching implications for the development of 20th century biology. In a similar manner, the decision to focus on chess as the measure of both human and computer intelligence had important and unintended consequences for AI research. This paper explores the emergence of chess as an experimental technology, its significance in the developing research practices of the AI community, and the unique ways in which the decision to focus on chess shaped the program of AI research in the decade of the 1970s. More broadly, it attempts to open up the virtual black box of computer software – and of computer games in particular – to the scrutiny of historical and sociological analysis.},
	pages = {5-30},
	language = {english},
	number = {1},
	journaltitle = {{Social Studies of Science}},
	shortjournal = {{Social Studies of Science}},
	author = {Ensmenger, Nathan},
	urldate = {2012-05-28},
	date = {2012-02-01},
	keywords = {{artificial intelligence}, {computing}, {drosophila}, {experimental technology}},
	file = {Is chess the drosophila of artificial intelligence? A social history of an algorithm:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/V3KGKEKN/5.html:text/html;Is chess the drosophila of artificial intelligence? A social history of an algorithm:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/SXFNK8TD/5.html:text/html}
}

% better-bibtex: generated
@video{Bacon_2012,
	title = {{Hilary Mason - Machine Learning for Hackers}},
	url = {http://vimeo.com/43547079},
	abstract = {Vimeo is the home for high-quality videos and the people who love them.},
	author = {{Bacon}},
	urldate = {2012-07-06},
	date = {2012-06-06},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/XXH9QQJS/43547079.html:text/html}
}

% better-bibtex: generated
@book{Koren_2009,
	title = {{Matrix Factorization Techniques for Recommender Systems}},
	abstract = {As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest-neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels. Modern consumers are inundated with choices. Electronic retailers and content providers offer a huge selection of products, with unprecedented opportunities to meet a variety of special needs and tastes. Matching consumers with the most appropriate products is key to enhancing user satisfaction and loyalty. Therefore, more retailers have become interested in recommender systems, which analyze patterns of user interest in products to provide personalized recommendations that suit a user’s taste. Because good personalized recommendations can add another dimension to the user experience, e-commerce leaders like Amazon.com and Netflix have made recommender systems a salient part of their websites. Such systems are particularly useful for entertainment products such as movies, music, and TV shows. Many customers will view the same movie, and each customer is likely to view numerous different movies. Customers have proven willing to indicate their level of satisfaction with particular movies, so a huge volume of data is available about which movies appeal to which customers. Companies can analyze this data to recommend movies to particular customers. Recommender system strategies Broadly speaking, recommender systems are based on one of two strategies. The content filtering approach creates a profile for each user or product to characterize its nature. For example, a movie profile could include attributes regarding its genre, the participating actors, its box office popularity, and so forth. User profiles might include demographic information or answers provided on a suitable questionnaire. The profiles allow programs to associate users with matching products. Of course, content-based strategies require gathering external information that might not be available or easy to collect. A known successful realization of content filtering is the Music Genome Project, which is used for the Internet radio service Pandora.com. A trained music analyst scores},
	author = {Koren, Yehuda and Bell, Robert and Volinsky, Chris},
	date = {2009},
	file = {Citeseer - Full Text PDF:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/KSM4IF25/Koren et al. - 2009 - Matrix Factorization Techniques for Recommender Sy.pdf:application/pdf;Citeseer - Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/RHNCXHWT/summary.html:text/html}
}

% better-bibtex: generated
% better-bibtex: soft conflict with _
@online{_a,
	title = {{dataists » A Taxonomy of Data Science}},
	url = {http://www.dataists.com/2010/09/a-taxonomy-of-data-science/},
	urldate = {2012-07-06},
	file = {dataists » A Taxonomy of Data Science:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/6VAB8TUH/a-taxonomy-of-data-science.html:text/html}
}

% better-bibtex: generated
% better-bibtex: soft conflict
@article{Wagstaff_2012,
	title = {{Machine Learning that Matters}},
	url = {http://arxiv.org/abs/1206.4656},
	abstract = {Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field?s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.},
	journaltitle = {{arXiv:1206.4656}},
	author = {Wagstaff, Kiri},
	urldate = {2012-07-16},
	date = {2012-06-18},
	keywords = {{Computer Science - Artificial Intelligence}, {Computer Science - Learning}, {Statistics - Machine Learning}},
	file = {1206.4656 PDF:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/UZRQSNZ9/Wagstaff - 2012 - Machine Learning that Matters.pdf:application/pdf;arXiv.org Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/XGPZ3HCN/1206.html:text/html}
}

% better-bibtex: generated
@article{Langley_2011,
	title = {{The changing science of machine learning}},
	volume = {{82}},
	url = {http://www.springerlink.com/index/J067H855N8223338.pdf},
	pages = {275–279},
	number = {3},
	journaltitle = {{Machine Learning}},
	author = {Langley, P.},
	urldate = {2012-06-21},
	date = {2011}
}

% better-bibtex: generated
@article{Carstens_2011,
	title = {{Sentiment Analysis}},
	url = {http://www.doc.ic.ac.uk/teaching/distinguished-projects/2011/l.carstens.pdf},
	author = {Carstens, L. and Intelligence, S. A},
	urldate = {2012-06-21},
	date = {2011}
}

% better-bibtex: generated
% better-bibtex: soft conflict with Wagstaff_2012
@article{Wagstaff_2012a,
	title = {{Machine Learning that Matters}},
	url = {http://ml.jpl.nasa.gov/papers/wagstaff/wagstaff-MLmatters-icml12.pdf},
	author = {Wagstaff, K. L},
	urldate = {2012-06-21},
	date = {2012}
}

% better-bibtex: generated
@book{Conway_2012,
	edition = {{1}},
	title = {{Machine Learning for Hackers}},
	isbn = {1449303714},
	pagetotal = {324},
	publisher = {{O'Reilly Media}},
	author = {Conway, Drew and White, John Myles},
	date = {2012}
}

% better-bibtex: generated
@book{Vapnik_1999,
	edition = {{2nd ed. 2000}},
	title = {{The Nature of Statistical Learning Theory}},
	isbn = {0387987800},
	pagetotal = {314},
	publisher = {{Springer}},
	author = {Vapnik, Vladimir},
	date = {1999-12-01}
}

% better-bibtex: generated
% better-bibtex: soft conflict
@inproceedings{Ma_2009,
	title = {{Identifying suspicious URLs: an application of large-scale online learning}},
	url = {http://dl.acm.org/citation.cfm?id=1553462},
	shorttitle = {{Identifying suspicious URLs}},
	pages = {681–688},
	booktitle = {{Proceedings of the 26th Annual International Conference on Machine Learning}},
	author = {Ma, Justin and Saul, Lawrence K. and Savage, Stefan and Voelker, Geoffrey M.},
	urldate = {2013-03-18},
	date = {2009},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/BS4JEB3N/citation.html:text/html}
}

% better-bibtex: generated
% better-bibtex: soft conflict with Ma_2009
@inproceedings{Ma_2009a,
	title = {{Beyond blacklists: learning to detect malicious web sites from suspicious URLs}},
	url = {http://dl.acm.org/citation.cfm?id=1557153},
	shorttitle = {{Beyond blacklists}},
	pages = {1245–1254},
	booktitle = {{Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining}},
	author = {Ma, Justin and Saul, Lawrence K. and Savage, Stefan and Voelker, Geoffrey M.},
	urldate = {2013-03-18},
	date = {2009},
	file = {[PDF] from sinica.edu.tw:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/V3934UNU/Ma et al. - 2009 - Beyond blacklists learning to detect malicious we.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/3GI8RQQN/citation.html:text/html}
}

% better-bibtex: generated
@article{Ma_2011,
	title = {{Learning to detect malicious URLs}},
	volume = {{2}},
	url = {http://dl.acm.org/citation.cfm?id=1961202},
	pages = {30},
	number = {3},
	journaltitle = {{ACM Transactions on Intelligent Systems and Technology (TIST)}},
	author = {Ma, Justin and Saul, Lawrence K. and Savage, Stefan and Voelker, Geoffrey M.},
	urldate = {2013-03-18},
	date = {2011},
	file = {[PDF] from berkeley.edu:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/BJ2GZCKP/Ma et al. - 2011 - Learning to detect malicious URLs.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/VS6CUR7Z/citation.html:text/html}
}

% better-bibtex: generated
@article{Le_2011,
	title = {{Building high-level features using large scale unsupervised learning}},
	url = {http://arxiv.org/abs/1112.6209},
	abstract = {We consider the problem of building high- level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 bil- lion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a clus- ter with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental re- sults reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bod- ies. Starting with these learned features, we trained our network to obtain 15.8\% accu- racy in recognizing 20,000 object categories from ImageNet, a leap of 70\% relative im- provement over the previous state-of-the-art.},
	journaltitle = {{arXiv:1112.6209}},
	author = {Le, Quoc V. and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
	urldate = {2013-04-21},
	date = {2011-12-28},
	keywords = {{Computer Science - Learning}},
	file = {1112.6209 PDF:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/QUF2DCRS/Le et al. - 2011 - Building high-level features using large scale uns.pdf:application/pdf;arXiv.org Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/TU8JVCMI/1112.html:text/html}
}

% better-bibtex: generated
@mvbook{Bishop_2006,
	title = {{Pattern recognition and machine learning}},
	volume = {{1}},
	url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
	publisher = {{springer New York}},
	author = {Bishop, Christopher M. and Nasrabadi, Nasser M.},
	urldate = {2013-07-12},
	date = {2006},
	file = {[PDF] from wisc.edu:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/DWIDSVQD/Bishop and Nasrabadi - 2006 - Pattern recognition and machine learning.pdf:application/pdf}
}

% better-bibtex: generated
@book{Murphy_2012,
	location = {{Cambridge, MA}},
	title = {{Machine learning: a probabilistic perspective}},
	isbn = {9780262018029  0262018020},
	shorttitle = {{Machine learning}},
	abstract = {"This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online"--Back cover.},
	language = {English},
	publisher = {{MIT Press}},
	author = {Murphy, Kevin P},
	date = {2012}
}

% better-bibtex: generated
@article{BBC_2012,
	title = {{Google 'brain' machine spots cats}},
	url = {http://www.bbc.co.uk/news/technology-18595351},
	abstract = {A Google research team has trained a network of 1,000 computers wired up like the human brain to recognise cats.},
	journaltitle = {{BBC News}},
	author = {{BBC}},
	urldate = {2013-06-06},
	date = {2012-06-26},
	file = {BBC News Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/GX9SSXDE/technology-18595351.html:text/html}
}

% better-bibtex: generated
% better-bibtex: soft conflict with Breiman_2001
@article{Breiman_2001a,
	title = {{Statistical modeling: The two cultures (with comments and a rejoinder by the author)}},
	volume = {{16}},
	url = {http://projecteuclid.org/euclid.ss/1009213726},
	shorttitle = {{Statistical modeling}},
	pages = {199–231},
	number = {3},
	journaltitle = {{Statistical Science}},
	author = {Breiman, Leo},
	urldate = {2013-06-10},
	date = {2001},
	file = {[PDF] from recognition.su:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/U23JA82S/Breiman - 2001 - Statistical modeling The two cultures (with comme.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/XNW8S5Q5/DPubS.html:text/html}
}

% better-bibtex: generated
% better-bibtex: soft conflict with StanfordUniversity_2008
@video{StanfordUniversity_2008a,
	title = {{Lecture 1 | Machine Learning (Stanford)}},
	url = {http://www.youtube.com/watch?v=UzxYlbK2c7E\&feature=youtube\_gdata\_player},
	abstract = {Lecture by Professor Andrew Ng for Machine Learning (CS 229) in the Stanford Computer Science department.  Professor Ng provides an overview of the course in this introductory meeting. 

This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include supervised learning, unsupervised learning, learning theory, reinforcement learning and adaptive control.   Recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing are also discussed.

Complete Playlist for the Course:
http://www.youtube.com/view\_play\_list?p=A89DCFA6ADACE599

CS 229 Course Website:
http://www.stanford.edu/class/cs229/

Stanford University:
http://www.stanford.edu/

Stanford University Channel on YouTube:
http://www.youtube.com/stanford},
	editora = {{StanfordUniversity}},
	editoratype = {collaborator},
	urldate = {2013-06-10},
	date = {2008-07-23}
}

% better-bibtex: generated
@article{Wacquant_2010,
	title = {{Participant Observation/Observant Participation}},
	url = {http://books.google.co.uk/books?hl=en\&lr=\&id=pLSAay\_xwjEC\&oi=fnd\&pg=PA69\&dq=wacquant+observant+\&ots=LUgGkfHYRD\&sig=WwrRprm32d0QB4H8LScW8zlINHc},
	pages = {69},
	journaltitle = {{Sociology: Introductory Readings}},
	author = {Wacquant, Loïc},
	urldate = {2013-06-11},
	date = {2010},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/JJ9N4N2X/books.html:text/html}
}

% better-bibtex: generated
@book{Barber_2011,
	location = {{Cambridge; New York}},
	title = {{Bayesian reasoning and machine learning}},
	isbn = {9780521518147 0521518148},
	abstract = {"Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online"-- "Vast amounts of data present amajor challenge to all thoseworking in computer science, and its many related fields, who need to process and extract value from such data. Machine learning technology is already used to help with this task in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis and robot locomotion. As its usage becomes more widespread, no student should be without the skills taught in this book. Designed for final-year undergraduate and graduate students, this gentle introduction is ideally suited to readers without a solid background in linear algebra and calculus. It covers everything from basic reasoning to advanced techniques in machine learning, and rucially enables students to construct their own models for real-world problems by teaching them what lies behind the methods. Numerous examples and exercises are included in the text. Comprehensive resources for students and instructors are available online"--},
	language = {English},
	publisher = {{Cambridge University Press}},
	author = {Barber, David},
	date = {2011}
}

% better-bibtex: generated
@book{Wacquant_2004,
	location = {{Oxford}},
	title = {{Body and Soul. Notebooks of an apprentice boxer}},
	publisher = {{Oxford University Press}},
	author = {Wacquant, Loic},
	date = {2004}
}

% better-bibtex: generated
@article{Olazaran_1996,
	title = {{A Sociological Study of the Official History of the Perceptrons Controversy}},
	volume = {{26}},
	issn = {0306-3127, 1460-3659},
	url = {http://sss.sagepub.com/content/26/3/611},
	doi = {10.1177/030631296026003005},
	abstract = {In this paper, I analyze the controversy within Artificial Intelligence (AI) which surrounded the `perceptron' project (and neural nets in general) in the late 1950s and early 1960s. I devote particular attention to the proofs and arguments of Minsky and Papert, which were interpreted as showing that further progress in neural nets was not possible, and that this approach to AI had to be abandoned. I maintain that this official interpretation of the debate was a result of the emergence, institutionalization and (importantly) legitimation of the symbolic AI approach (with its resource allocation system and authority structure). At the `research-area' level, there was considerable interpretative flexibility. This interpretative flexibility was further demonstrated by the revival of neural nets in the late 1980s, and subsequent rewriting of the official history of the debate.},
	pages = {611-659},
	language = {english},
	number = {3},
	journaltitle = {{Social Studies of Science}},
	shortjournal = {{Social Studies of Science}},
	author = {Olazaran, Mikel},
	urldate = {2013-06-17},
	date = {1996},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/U5C4XRIM/611.html:text/html}
}

% better-bibtex: generated
@online{Ng_,
	title = {{| Machine Learning III: Linear Algebra Review}},
	url = {https://class.coursera.org/ml-003/lecture/},
	abstract = {Video Lecture:  in Machine Learning on Coursera.},
	titleaddon = {{Coursera}},
	author = {Ng, Andrew},
	urldate = {2013-06-14},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/N72CWJG4/index.html:text/html}
}

% better-bibtex: generated
@article{Rosenblatt_1958,
	title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
	volume = {{65}},
	rights = {{(c) 2012 APA, all rights reserved}},
	issn = {1939-1471(Electronic);0033-295X(Print)},
	doi = {10.1037/h0042519},
	shorttitle = {{The perceptron}},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references.},
	pages = {386-408},
	number = {6},
	journaltitle = {{Psychological Review}},
	author = {Rosenblatt, F.},
	date = {1958},
	keywords = {{brain}, {information storage}, {probabilistic model}}
}

% better-bibtex: generated
@article{Minsky_1969,
	title = {{Perceptron: an introduction to computational geometry}},
	volume = {{19}},
	shorttitle = {{Perceptron}},
	pages = {88},
	journaltitle = {{The MIT Press, Cambridge, expanded edition}},
	author = {Minsky, Marvin and Papert, Seymour},
	date = {1969}
}

% better-bibtex: generated
@online{Dahl_,
	title = {{Deep Learning How I Did It: Merck 1st place interview}},
	url = {http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/},
	shorttitle = {{Deep Learning How I Did It}},
	abstract = {What was your background prior to entering this challenge? We are a team of computer science and statistics academics. Ruslan Salakhutdinov and Geoff Hinton are professors at the University of Toro...},
	titleaddon = {{no free hunch}},
	author = {Dahl, George},
	urldate = {2013-06-17},
	keywords = {{code}, {geoff hinton}, {learning}, {machine}, {merck}, {model}, {neural}, {professor hinton}},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/2M52PT7J/deep-learning-how-i-did-it-merck-1st-place-interview.html:text/html}
}

% better-bibtex: generated
@article{Hinton_2006,
	title = {{Reducing the dimensionality of data with neural networks}},
	volume = {{313}},
	url = {http://www.sciencemag.org/content/313/5786/504.short},
	pages = {504–507},
	number = {5786},
	journaltitle = {{Science}},
	author = {Hinton, Geoffrey E. and Salakhutdinov, Ruslan R.},
	urldate = {2013-06-17},
	date = {2006},
	file = {[PDF] from uni-saarland.de:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/KGM75BT4/Hinton and Salakhutdinov - 2006 - Reducing the dimensionality of data with neural ne.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/WRD6BG4A/504.html:text/html}
}

% better-bibtex: generated
@article{Ackley_1985,
	title = {{A learning algorithm for Boltzmann machines}},
	volume = {{9}},
	url = {http://www.sciencedirect.com/science/article/pii/S0364021385800124},
	pages = {147–169},
	number = {1},
	journaltitle = {{Cognitive science}},
	author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
	urldate = {2013-06-17},
	date = {1985},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/G6CJZC4J/S0364021385800124.html:text/html}
}

% better-bibtex: generated
@online{KDD_2013,
	title = {{Call For KDD Cup}},
	url = {http://www.kdd.org/kdd2013/call-for-cup},
	author = {{KDD}},
	urldate = {2013-07-23},
	date = {2013},
	file = {Call For KDD Cup | http://www.kdd.org/kdd2013:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/WG5FVTJU/call-for-cup.html:text/html}
}

% better-bibtex: generated
@inreference{_2013,
	title = {{Perceptron}},
	rights = {{Creative Commons Attribution-ShareAlike License}},
	url = {http://en.wikipedia.org/w/index.php?title=Perceptron\&oldid=557301943},
	abstract = {In computational geometry, the perceptron is an algorithm for supervised classification of an input into one of several possible non-binary outputs. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector describing a given input using the delta rule. The learning algorithm for perceptrons is an online algorithm, in that it processes elements in the training set one at a time.},
	language = {english},
	booktitle = {{Wikipedia, the free encyclopedia}},
	urldate = {2013-06-17},
	date = {2013-05-29},
	note = {Page Version ID: 557301943},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/P3MWEXNI/index.html:text/html}
}

% better-bibtex: generated
@article{Patil_2010,
	title = {{PyMC: Bayesian stochastic modelling in Python}},
	volume = {{35}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/},
	shorttitle = {{PyMC}},
	pages = {1},
	number = {4},
	journaltitle = {{Journal of statistical software}},
	author = {Patil, Anand and Huard, David and Fonnesbeck, Christopher J.},
	urldate = {2013-06-17},
	date = {2010},
	file = {[HTML] from nih.gov:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/3NWB7TGN/PMC3097064.html:text/html}
}

% better-bibtex: generated
@video{Koller_2012,
	title = {{Daphne Koller: What we're learning from online education | Video on TED.com}},
	url = {http://www.ted.com/talks/daphne\_koller\_what\_we\_re\_learning\_from\_online\_education.html},
	shorttitle = {{Daphne Koller}},
	abstract = {Daphne Koller is enticing top universities to put their most intriguing courses online for free -- not just as a service, but as a way to research how people learn. With Coursera (cofounded by Andrew Ng), each keystroke, quiz, peer-to-peer discussion and self-graded assignment builds an unprecedented pool of data on how knowledge is processed.},
	editora = {Koller, Daphne},
	editoratype = {collaborator},
	urldate = {2013-06-24},
	date = {2012-08},
	keywords = {{Computers}, {education}, {global issues}, {Internet}, {Talks}, {TED}},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/83HZTXGW/daphne_koller_what_we_re_learning_from_online_education.html:text/html}
}

% better-bibtex: generated
@book{Alpaydin_2010,
	location = {{Cambridge, Massachusetts; London}},
	title = {{Introduction to machine learning}},
	isbn = {9780262012430  026201243X},
	language = {English},
	publisher = {{The MIT Press}},
	author = {Alpaydin, E},
	date = {2010}
}

% better-bibtex: generated
@book{Rasmussen_2006,
	location = {{Cambridge, Mass.}},
	title = {{Gaussian processes for machine learning}},
	isbn = {026218253X 9780262182539},
	abstract = {"Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics."--Jacket.},
	language = {English},
	publisher = {{MIT Press}},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I},
	date = {2006}
}

% better-bibtex: generated
@book{Marsland_2009,
	location = {{Boca Raton, Mass. \[u.a.}},
	title = {{Machine learning: an algorithmic perspective}},
	isbn = {9781420067187  1420067184},
	shorttitle = {{Machine learning}},
	language = {English},
	publisher = {{CRC Press/Taylor \& Francis}},
	author = {Marsland, Stephen},
	date = {2009}
}

% better-bibtex: generated
@book{Mitchell_1997,
	location = {{New York, NY \[u.a.}},
	title = {{Machine learning}},
	isbn = {0071154671 9780071154673},
	language = {English},
	publisher = {{McGraw-Hill}},
	author = {Mitchell, Tom M},
	date = {1997}
}

% better-bibtex: generated
@article{Ward_2006,
	title = {{Short-term prediction of mortality in patients with systemic lupus erythematosus: Classification of outcomes using random forests}},
	volume = {{55}},
	rights = {{Copyright © 2006 by the American College of Rheumatology}},
	issn = {1529-0131},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/art.21695/abstract},
	doi = {10.1002/art.21695},
	shorttitle = {{Short-term prediction of mortality in patients with systemic lupus erythematosus}},
	abstract = {ObjectiveTo identify demographic and clinical characteristics that classify patients with systemic lupus erythematosus (SLE) at risk for in-hospital mortality.MethodsPatients hospitalized in California from 1996 to 2000 with a principal diagnosis of SLE (N = 3,839) were identified from a state hospitalization database. As candidate predictors of mortality, we used patient demographic characteristics; the presence or absence of 40 different clinical conditions listed among the discharge diagnoses; and 2 summary indexes derived from the discharge diagnoses, the Charlson Index and the SLE Comorbidity Index. Predictors of patients at increased risk of mortality were identified and validated using random forests, a statistical procedure that is a generalization of single classification trees. Random forests use bootstrapped samples of patients and randomly selected subsets of predictors to create individual classification trees, and this process is repeated to generate multiple trees (a forest). Classification is then done by majority vote across all trees.ResultsOf the 3,839 patients, 109 died during hospitalization. Selecting from all available predictors, the random forests had excellent predictive accuracy for classification of death. The mean classification error rate, averaged over 10 forests of 500 trees each, was 11.9\%. The most important predictors were the Charlson Index, respiratory failure, SLE Comorbidity Index, age, sepsis, nephritis, and thrombocytopenia.ConclusionInformation on clinical diagnoses can be used to accurately predict mortality among hospitalized patients with SLE. Random forests represent a useful technique to identify the most important predictors from a larger (often much larger) number and to validate the classification.},
	pages = {74–80},
	language = {english},
	number = {1},
	journaltitle = {{Arthritis Care \& Research}},
	author = {Ward, Michael M. and Pajevic, Sinisa and Dreyfuss, Jonathan and Malley, James D.},
	urldate = {2013-07-16},
	date = {2006},
	keywords = {{Classification tree}, {Hospitalization}, {Mortality}, {Random forest}, {Systemic lupus erythematosus}},
	file = {Full Text PDF:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/TC4ZHWXS/Ward et al. - 2006 - Short-term prediction of mortality in patients wit.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/DIDEBIB3/abstract.html:text/html}
}

% better-bibtex: generated
@article{Steinberg_2009,
	title = {{CART: classification and regression trees}},
	url = {http://books.google.co.uk/books?hl=en\&lr=\&id=\_kcEn-c9kYAC\&oi=fnd\&pg=PA179\&dq=dan+steinberg+cart\&ots=eQ7jtfUODm\&sig=Xs8kegu\_D4DcrPhT6TUkB0LCV1A},
	shorttitle = {{CART}},
	pages = {179–201},
	journaltitle = {{The Top Ten Algorithms in Data Mining}},
	author = {Steinberg, Dan and Colla, Phillip},
	urldate = {2013-09-12},
	date = {2009},
	note = {00245},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/CMM3XGQE/books.html:text/html}
}

% better-bibtex: generated
@article{Morgan_1963,
	title = {{Problems in the analysis of survey data, and a proposal}},
	volume = {{58}},
	url = {http://amstat.tandfonline.com/doi/full/10.1080/01621459.1963.10500855},
	pages = {415–434},
	number = {302},
	journaltitle = {{Journal of the American Statistical Association}},
	author = {Morgan, James N. and Sonquist, John A.},
	urldate = {2013-09-17},
	date = {1963},
	note = {00873},
	file = {[PDF] from uiuc.edu:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/8UI33RE4/Morgan and Sonquist - 1963 - Problems in the analysis of survey data, and a pro.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/7HCASV3V/01621459.1963.html:text/html}
}

% better-bibtex: generated
@article{Cortes_1995,
	title = {{Support-Vector Networks}},
	volume = {{20}},
	issn = {0885-6125},
	doi = {10.1023/A:1022627411411},
	abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
	pages = {273-297},
	language = {English},
	number = {3},
	journaltitle = {{Machine Learning}},
	shortjournal = {{Mach. Learn.}},
	author = {Cortes, C. and Vapnik, V.},
	date = {1995-09},
	note = {11952},
	keywords = {{efficient learning algorithms}, {neural networks}, {pattern recognition}, {polynomial classifiers}, {radial basis function classifiers}}
}

% better-bibtex: generated
@article{Einhorn_1972,
	title = {{Alchemy in the Behavioral Sciences}},
	volume = {{36}},
	issn = {0033-362X, 1537-5331},
	url = {http://poq.oxfordjournals.org/content/36/3/367},
	doi = {10.1086/268019},
	abstract = {Access to powerful new computers has encouraged routine use of highly complex analytic techniques, often in the absence of any theory, hypotheses, or model to guide the researcher's expectations of results. The author examines the potential of such techniques for generating spurious results, and urges that in exploratory work the outcome be subjected to a more rigorous criterion than the usual tests of statistical significance.},
	pages = {367-378},
	language = {english},
	number = {3},
	journaltitle = {{Public Opinion Quarterly}},
	shortjournal = {{Public Opin Q}},
	author = {Einhorn, Hillel J.},
	urldate = {2013-09-18},
	date = {1972-09-21},
	note = {00060},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/X3J3BMFX/367.html:text/html}
}

% better-bibtex: generated
@article{Breiman_1984,
	title = {{CART: Classification and regression trees}},
	volume = {{156}},
	shorttitle = {{CART}},
	journaltitle = {{Wadsworth: Belmont, CA}},
	author = {Breiman, Leo and Friedman, Jerome and Olshen, Richard and Stone, Charles and Steinberg, D. and Colla, P.},
	date = {1984},
	note = {00004}
}

% better-bibtex: generated
@article{Doyle_1973,
	title = {{The use of automatic interaction detector and similar search procedures}},
	url = {http://www.jstor.org/stable/10.2307/3008131},
	pages = {465–467},
	journaltitle = {{Operational Research Quarterly}},
	author = {Doyle, Peter},
	urldate = {2013-09-18},
	date = {1973},
	note = {00056},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/DXU7KQHG/3008131.html:text/html}
}

% better-bibtex: generated
@article{Cover_1967,
	title = {{Nearest neighbor pattern classification}},
	volume = {{13}},
	url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1053964},
	pages = {21–27},
	number = {1},
	journaltitle = {{Information Theory, IEEE Transactions on}},
	author = {Cover, Thomas and Hart, Peter},
	urldate = {2013-09-19},
	date = {1967},
	note = {05055},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/AQB243F7/login.html:text/html}
}

% better-bibtex: generated
@article{Friedman_1977,
	title = {{Recursive Partitioning Decision Rule for Nonparametric Classification}},
	volume = {{26}},
	pages = {404-408},
	number = {4},
	journaltitle = {{Ieee Transactions on Computers}},
	author = {Friedman, Jh},
	date = {1977},
	note = {00364}
}

% better-bibtex: generated
@article{Quinlan_1986,
	title = {{Induction of decision trees}},
	volume = {{1}},
	url = {http://link.springer.com/article/10.1023/A:1022643204877},
	pages = {81–106},
	number = {1},
	journaltitle = {{Machine learning}},
	author = {Quinlan, J. Ross},
	urldate = {2013-09-25},
	date = {1986},
	note = {12427},
	file = {[PDF] from googlecode.com:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/3VG28M3F/Quinlan - 1986 - Induction of decision trees.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/MAF3CH37/A1022643204877.html:text/html}
}

% better-bibtex: generated
@article{Domingos_2012,
	title = {{A few useful things to know about machine learning}},
	volume = {{55}},
	url = {http://dl.acm.org/citation.cfm?id=2347755},
	pages = {78–87},
	number = {10},
	journaltitle = {{Communications of the ACM}},
	author = {Domingos, Pedro},
	urldate = {2013-09-25},
	date = {2012},
	note = {00027},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/K92BG2X7/citation.html:text/html}
}

% better-bibtex: generated
@mvbook{Quinlan_1993,
	title = {{C4. 5: programs for machine learning}},
	volume = {{1}},
	url = {http://books.google.co.uk/books?hl=en\&lr=\&id=HExncpjbYroC\&oi=fnd\&pg=PR7\&dq=quinlan+c4.5\&ots=nKr8dYr51o\&sig=e8CDMoHt0VGemFkhf8z9VMA8KlI},
	shorttitle = {{C4. 5}},
	publisher = {{Morgan kaufmann}},
	author = {Quinlan, John Ross},
	urldate = {2013-09-26},
	date = {1993},
	note = {22318},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/N6GSGR8B/books.html:text/html}
}

% better-bibtex: generated
@mvbook{Bellman_1961,
	title = {{Adaptive control processes: a guided tour}},
	volume = {{4}},
	url = {http://www.getcited.org/pub/101191710},
	shorttitle = {{Adaptive control processes}},
	publisher = {{Princeton university press Princeton}},
	author = {Bellman, Richard},
	urldate = {2013-09-30},
	date = {1961},
	note = {02427},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/JA2JAC9D/101191710.html:text/html}
}

% better-bibtex: generated
@article{Chang_2011,
	title = {{LIBSVM: a library for support vector machines}},
	volume = {{2}},
	url = {http://dl.acm.org.ezproxy.lancs.ac.uk/citation.cfm?id=1961199},
	shorttitle = {{LIBSVM}},
	pages = {27},
	number = {3},
	journaltitle = {{ACM Transactions on Intelligent Systems and Technology (TIST)}},
	author = {Chang, Chih-Chung and Lin, Chih-Jen},
	urldate = {2013-10-04},
	date = {2011},
	note = {14513},
	file = {[PDF] from 140.112.30.28:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/9X4U49SA/Chang and Lin - 2011 - LIBSVM a library for support vector machines.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/K9XNAVFS/citation.html:text/html}
}

% better-bibtex: generated
@article{Burges_1998,
	title = {{A tutorial on support vector machines for pattern recognition}},
	volume = {{2}},
	url = {http://link.springer.com.ezproxy.lancs.ac.uk/article/10.1023/A:1009715923555},
	pages = {121–167},
	number = {2},
	journaltitle = {{Data mining and knowledge discovery}},
	author = {Burges, Christopher JC},
	urldate = {2013-10-04},
	date = {1998},
	note = {12162},
	file = {[PDF] from mingzeng.net:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/29STDTR3/Burges - 1998 - A tutorial on support vector machines for pattern .pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/MQ5JNWFJ/A1009715923555.html:text/html}
}

% better-bibtex: generated
@article{Fisher_1936,
	title = {{The use of multiple measurements in taxonomic problems}},
	volume = {{7}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/full},
	pages = {179–188},
	number = {2},
	journaltitle = {{Annals of eugenics}},
	author = {Fisher, Ronald A.},
	urldate = {2013-10-04},
	date = {1936},
	note = {08904},
	file = {[PDF] from adelaide.edu.au:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/IIB6XP2G/Fisher - 1936 - The use of multiple measurements in taxonomic prob.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/MUEZJMAQ/full.html:text/html}
}

% better-bibtex: generated
@article{Vapnik_1971,
	title = {{On the uniform convergence of relative frequencies of events to their probabilities}},
	volume = {{16}},
	url = {http://epubs.siam.org/doi/abs/10.1137/1116025},
	pages = {264–280},
	number = {2},
	journaltitle = {{Theory of Probability \& Its Applications}},
	author = {Vapnik, Vladimir N. and Chervonenkis, A. Ya},
	urldate = {2013-10-11},
	date = {1971},
	note = {02515},
	file = {[PDF] from zju88.org:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/J7TCIZWC/Vapnik and Chervonenkis - 1971 - On the uniform convergence of relative frequencies.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/IF97DCJ2/1116025.html:text/html}
}

% better-bibtex: generated
@article{Pedregosa_2011,
	title = {{Scikit-learn: Machine Learning in Python}},
	volume = {{12}},
	pages = {2825–2830},
	journaltitle = {{Journal of Machine Learning Research}},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	date = {2011},
	note = {00285}
}

% better-bibtex: generated
@article{Valiant_1984,
	title = {{A theory of the learnable}},
	volume = {{27}},
	url = {http://dl.acm.org/citation.cfm?id=1972},
	pages = {1134–1142},
	number = {11},
	journaltitle = {{Communications of the ACM}},
	author = {Valiant, Leslie G.},
	urldate = {2013-12-16},
	date = {1984},
	note = {04407},
	file = {[PDF] from iitk.ac.in:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/C43SG9FS/Valiant - 1984 - A theory of the learnable.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/GPXTHEXZ/citation.html:text/html}
}

% better-bibtex: generated
@book{Jain_2008,
	title = {{Data Clustering: 50 Years Beyond K-Means}},
	shorttitle = {{Data Clustering}},
	abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into taxonomic ranks: domain, kingdom, phylum, class, etc.). Cluster analysis is the formal study of algorithms and methods for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is exploratory in nature to find structure in data. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty of designing a general purpose clustering algorithm and the illposed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection, and data clustering and large scale data clustering.},
	author = {Jain, Anil K.},
	date = {2008},
	note = {00044},
	file = {Citeseer - Full Text PDF:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/U56GQPVZ/Jain - 2008 - Data Clustering 50 Years Beyond K-Means.pdf:application/pdf;Citeseer - Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/B2IQPERU/summary.html:text/html}
}

% better-bibtex: generated
@book{Solovey_2012,
	title = {{Cold War Social Science: Knowledge Production, Liberal Democracy, and Human Nature}},
	isbn = {9781137013224},
	shorttitle = {{Cold War Social Science}},
	abstract = {From World War II to the early 1970s, social science research expanded in dramatic and unprecedented fashion in the United States, which became the world's acknowledged leader in the field. This volume examines how, why, and with what consequences this rapid and yet contested expansion depended on the entanglement of the social sciences with the Cold War. Utilizing the controversial but useful concept of "Cold War Social Science," the contributions gathered here reveal how scholars from established disciplines and new interdisciplinary fields of study made important contributions to long-standing debates about knowledge production, liberal democracy, and human nature in an era of diplomatic tension and ideological conflict.},
	pagetotal = {289},
	language = {english},
	publisher = {{Palgrave Macmillan}},
	author = {Solovey, Mark and Cravens, Hamilton},
	date = {2012},
	note = {00006},
	keywords = {{History / General}, {History / Social History}, {History / United States / 20th Century}, {Political Science / History \& Theory}, {Political Science / Political Ideologies / Democracy}, {SOCIAL SCIENCE / Methodology}, {SOCIAL SCIENCE / Research}}
}

% better-bibtex: generated
@article{Flach_2001,
	title = {{On the state of the art in machine learning: A personal review}},
	volume = {{131}},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370201001254},
	shorttitle = {{On the state of the art in machine learning}},
	pages = {199–222},
	number = {1},
	journaltitle = {{Artificial Intelligence}},
	author = {Flach, Peter A.},
	urldate = {2014-03-26},
	date = {2001},
	file = {[HTML] from sciencedirect.com:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/ZHP82QBG/S0004370201001254.html:text/html}
}