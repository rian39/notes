# Malley, Statistical Learning
malley_statistical_2011,
	edition = {1},
	title = {Statistical Learning for Biomedical Data},

working with statistical learning machines can push us to think about novel structures and functions in our data. This awareness is often counterintuitive, and familiar methods such as simple correlations, or slightly more evolved partial correlations, are often not sufficience to pin down these deeper connections. 5-6

A statistical learning machine is a procedure that can potentially make good use of difficult data. And _if_ it is shown to be effective in making a good prediction about a clinical process or outcome, it can lead us to refined understanding of that porcess: it can help us _learn_ about the process. 6

most of the procedures are reinforcing in their conclusions and are (often) nearly trivial to implement 6

logistic regression is the default "simple" model for predicting a subject's group status. ... it can be applied after a more complex learning machine has done the heavy lifting of identifying an important set of predictors given a very large list of candidate predictors. It could also be inlcuded as a "measurement" in a learning machine, so that predictions from one machine can be into the input for other machines.  43

## naive bayes

A Bayes classifier is a probabilistic learnign scheme that begins with a choice of priors. Priors are probabilities for group membership before data is collected, and often, more grandly, also probabilities for different models or states of Nature. Then an argument from classical probability generates the conditional probability of observing a particular outcome _given_ the predictors and priors. 45

We still often see that a Bayes classifier requires larger amounts of data that other methods to achieve the same level of model accuracy. 45

naive Bayes .. assumes statistical independence of all the features: at a very minimum this means the features are assumed to be all uncorrelated with each other. ... This is often a wildly implausible assumption but the method has been shown to work surprisingly well 46

the method constrains the probability of observing a particular outcome given a set of predictors to just a simple _product_ of the probabilities conditioned on individual predictors. ... Each of the conditional probabilities  ... is estimated from the data and the product yields the estimated probability of $y$. 46

A warning: in order to make this method computationally efficient, especially with large numbers of predictors, it is usually assumed that the predictors are themselves discrete, or can be made discrete in some sensible way. 46

The logistic regression model ... is a widely used statistical technique in biomedical data analysis, for a number of reasons. 91

Mastering the details of tree growth and management is an excellent way to understand the activities of learning machine generally. 118

Given data, a tree is generated by  first selecting a feature from the full list of features, and the selection is occurs at every node under consideration for splitting. Often the feature list is very big.  ... However, no distributional assumptions or other statistical premises are made in decision trees concerning the features or the subjects. 120

Selecting a single feature for consideration is done randomly from the full list of predictors. 120

There is a basic tree idea at work here: how a tree _looks_, complex, simple or something in between, is not a reliable indication of its predictive ability. 125

What a tree does at the very top is almost certainly not a good way to assess how it does by the time it gets to the bottom. And it is only at the bottom, where the terminal nodes appears, that the tree is making decisions on new cases. 136

Firstly, if the data has any signal at all then most machines will detect it. 257

we freely admit that many machines studied in this text are somewhat mysterious, though powerful engines.  257
